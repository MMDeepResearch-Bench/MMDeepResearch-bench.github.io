<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>MMDR-Bench</title>
  <meta name="description" content="MMDeepResearch-Bench: Grounded Evaluation & Alignment for Multimodal Deep Research Agents." />

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">

  <style>
    :root{
      --bg:#ffffff;
      --fg:#0f172a;
      --muted:#475569;
      --muted2:#64748b;
      --border:#e2e8f0;
      --soft:#f8fafc;
      --card:#ffffff;
      --dark:#0b1220;
      --shadow:0 12px 28px rgba(2, 6, 23, .08);
      --radius:14px;
      --max:1120px;
      --max-content:980px;
    }
    *{ box-sizing:border-box; }
    body{
      margin:0;
      font-family: Inter, "Noto Sans SC", system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
      color:var(--fg);
      background:var(--bg);
      line-height:1.65;
    }
    a{ color:inherit; text-decoration:none; }
    a:hover{ text-decoration:underline; }

    .container{ max-width:var(--max); margin:0 auto; padding:0 18px; }
    .is-max-desktop{ max-width:var(--max-content); margin:0 auto; }
    .has-text-centered{ text-align:center; }
    .has-text-justified{ text-align:justify; }
    .content p{ margin:0 0 14px; }
    .muted{ color:var(--muted); }
    .muted2{ color:var(--muted2); }

    /* Top nav */
    header{ border-bottom:1px solid var(--border); background:#fff; }
    .nav{
      max-width:var(--max);
      margin:0 auto;
      padding:10px 18px;
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
    }
    .brand{
      display:flex;
      align-items:center;
      gap:12px;
      font-weight:900;
      letter-spacing:-.01em;
    }
    .brand-logo {
      height: 32px;
      width: auto;
      border-radius: 6px;
      display: block;
    }
    .navlinks{
      display:flex;
      flex-wrap:wrap;
      gap:10px;
      align-items:center;
      justify-content:flex-end;
      color:var(--muted);
      font-weight:800;
      font-size:14px;
    }
    .navlinks a{
      padding:6px 10px;
      border-radius:10px;
    }
    .navlinks a:hover{
      background:var(--soft);
      text-decoration:none;
      color:var(--fg);
    }
    @media (max-width: 880px){
      .navlinks{ display:none; }
    }

    /* Hero */
    .hero-wrap{
      background:
        radial-gradient(900px 260px at 15% 0%, rgba(79,70,229,.10), transparent 60%),
        radial-gradient(900px 260px at 85% 0%, rgba(6,182,212,.10), transparent 60%),
        linear-gradient(180deg, var(--soft), #ffffff 55%);
      border-bottom:1px solid var(--border);
    }
    .hero{
      max-width:var(--max);
      margin:0 auto;
      padding:54px 18px 40px;
    }
    .mark{
      display:inline-flex;
      align-items:center;
      gap:14px;
      font-weight:900;
      letter-spacing:-.02em;
      font-size:44px;
      line-height:1;
      margin-bottom:10px;
    }
    .hero-logo {
      height: 56px;
      width: auto;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(2,6,23,.1);
      background: white;
    }
    .title{
      margin:0 auto;
      font-size: clamp(22px, 3.2vw, 34px);
      line-height:1.22;
      font-weight:700;
      max-width: 980px;
      letter-spacing:-.01em;
    }
    
    /* Author Section Styling */
    .authors-wrapper {
      margin: 20px auto 0;
      max-width: 980px;
      text-align: center;
    }
    .authors {
      color: #0f172a; /* Êõ¥Ê∑±ÁöÑÈ¢úËâ≤ÔºåÁ™ÅÂá∫‰ΩúËÄÖ */
      font-size: 16px;
      font-weight: 500;
      line-height: 1.6;
    }
    .authors a { color: inherit; text-decoration: none; }
    .authors a:hover { text-decoration: underline; }
    
    .team-label {
      margin-top: 10px;
      font-size: 17px;
      font-weight: 800;
      color: var(--fg);
    }
    
    .contribution-notes {
      margin-top: 8px;
      font-size: 13px;
      color: var(--muted2);
      display: flex;
      justify-content: center;
      gap: 16px;
      flex-wrap: wrap;
    }

    /* Buttons */
    .publication-links{
      display:flex;
      gap:12px;
      flex-wrap:wrap;
      align-items:center;
      justify-content:center;
      margin-top:24px;
    }
    .button{
      display:inline-flex;
      align-items:center;
      gap:8px;
      padding:10px 16px;
      border-radius:999px;
      border:1px solid var(--dark);
      background:var(--dark);
      color:#fff;
      font-weight:900;
      font-size:14px;
      line-height:1;
      box-shadow:0 1px 0 rgba(2,6,23,.06);
    }
    .button:hover{ text-decoration:none; filter:brightness(1.06); color:#fff; }
    .button .icon{
      width:18px; height:18px;
      display:inline-flex;
      align-items:center;
      justify-content:center;
      font-size:15px;
    }

    .meta-row{
      display:flex;
      flex-wrap:wrap;
      gap:10px;
      justify-content:center;
      margin-top:16px;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:6px;
      font-size:12px;
      font-weight:900;
      padding:4px 10px;
      border-radius:999px;
      border:1px solid var(--border);
      background: rgba(248,250,252,.85);
      color: var(--muted);
    }

    section{
      max-width: var(--max);
      margin: 0 auto;
      padding: 42px 18px;
    }
    h2{
      font-size: 26px;
      margin: 0 0 14px;
      letter-spacing: -.2px;
    }
    h3{
      margin: 0 0 8px;
      font-size: 16px;
      letter-spacing: -.2px;
    }

    /* band separators */
    .band{
      border-top:1px solid var(--border);
      border-bottom:1px solid var(--border);
      background:var(--soft);
      padding:24px 18px;
    }
    .band-inner{
      max-width:var(--max);
      margin:0 auto;
      text-align:center;
    }
    .band-title{
      margin:0;
      font-size:32px;
      letter-spacing:-.02em;
      font-weight:900;
    }

    /* Grid & Cards */
    .grid{
      display:grid;
      grid-template-columns: repeat(12, 1fr);
      gap:14px;
    }
    .card{
      background:var(--card);
      border:1px solid var(--border);
      border-radius:var(--radius);
      padding:16px;
      box-shadow:var(--shadow);
    }
    figure{ margin:0; }
    .figure-card{
      background:var(--soft);
      border:1px solid var(--border);
      border-radius:var(--radius);
      padding:14px;
    }
    .figure-media{
      background:#fff;
      border:1px solid var(--border);
      border-radius:12px;
      padding:18px;
      display:flex;
      align-items:center;
      justify-content:center;
      overflow:hidden;
    }
    .paper-fig{
      display:block;
      max-width:100%;
      height:auto;
      border-radius:10px;
    }
    figcaption{
      margin-top:10px;
      color:var(--muted2);
      font-size:13px;
      font-weight:550;
    }

    /* Leaderboard Styles */
    .leaderboard-container {
      background: white;
      border: 1px solid var(--border);
      border-radius: var(--radius);
      overflow: hidden;
      box-shadow: var(--shadow);
      margin-top: 14px;
    }
    .leaderboard-header {
      background: var(--dark);
      color: white;
      padding: 22px 24px;
    }
    .leaderboard-header h3 {
      margin: 0 0 4px;
      font-size: 20px;
      color: white;
      font-weight: 900;
      letter-spacing: -.01em;
    }
    .leaderboard-header p {
      margin: 0;
      opacity: 0.95;
      font-size: 14px;
      color: white;
      font-weight: 600;
    }
    .controls {
      padding: 16px 20px;
      background: var(--soft);
      border-bottom: 1px solid var(--border);
      display: flex; gap: 12px; flex-wrap: wrap; align-items: center;
    }
    .filter-group { display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
    .filter-btn {
      padding: 6px 14px; border: 1px solid var(--border); background: white;
      border-radius: 999px; cursor: pointer; font-size: 13px;
      transition: all 0.2s; font-weight: 800; color: var(--fg);
    }
    .filter-btn:hover { border-color: var(--dark); color: var(--dark); }
    .filter-btn.active { background: var(--dark); color: white; border-color: var(--dark); }
    .search-box { flex: 1; min-width: 200px; max-width: 320px; }
    .search-box input {
      width: 100%; padding: 8px 12px; border: 1px solid var(--border);
      border-radius: 999px; font-size: 13px; outline: none; font-weight: 600;
    }
    .search-box input:focus{ border-color: rgba(2,6,23,.35); box-shadow: 0 0 0 3px rgba(2,6,23,.10); }
    
    .metrics-legend {
      padding: 16px 20px;
      background: #fff;
      border-bottom: 1px solid var(--border);
      font-size: 13px;
      color: var(--muted);
    }
    .legend-title { font-weight: 900; margin-bottom: 8px; display: block; color: var(--fg); }
    .legend-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 12px;
    }
    .legend-item { display: flex; align-items: baseline; gap: 6px; line-height: 1.4; }
    .legend-badge { 
      background: var(--dark); color: white; padding: 2px 6px; border-radius: 4px; 
      font-weight: 700; font-size: 11px; flex-shrink: 0;
    }

    .table-container { overflow-x: auto; }
    .leaderboard-table { width: 100%; border-collapse: collapse; font-size: 13px; }
    .leaderboard-table thead { background: var(--soft); position: sticky; top: 0; z-index: 10; }
    .leaderboard-table th {
      padding: 10px 8px; text-align: center; font-weight: 900; color: var(--muted);
      border-bottom: 2px solid var(--border); white-space: nowrap; cursor: pointer; user-select: none; font-size: 12px;
    }
    .leaderboard-table th:hover { background: #eef2ff; }
    .leaderboard-table th.sortable::after { content: ' ‚áÖ'; opacity: 0.3; font-size: 10px; }
    .leaderboard-table th.sort-asc::after { content: ' ‚Üë'; opacity: 1; }
    .leaderboard-table th.sort-desc::after { content: ' ‚Üì'; opacity: 1; }
    .metric-group { border-left: 2px solid var(--border); }
    .leaderboard-table td { padding: 10px 8px; text-align: center; border-bottom: 1px solid #f1f3f5; font-weight: 600; }
    .leaderboard-table tr:hover { background: var(--soft); }
    .model-cell {
      text-align: left !important; font-weight: 800; padding-left: 12px !important;
      position: sticky; left: 0; background: white; z-index: 5; min-width: 240px;
    }
    .leaderboard-table tr:hover .model-cell { background: var(--soft); }
    .company-logo {
      display: inline-block; margin-right: 8px; vertical-align: middle;
      width: 20px; height: 20px; object-fit: contain; opacity: 0.9;
      transition: opacity 0.2s, transform 0.2s; flex-shrink: 0; border-radius: 4px;
    }
    .model-cell:hover .company-logo { opacity: 1; transform: scale(1.05); }
    .rank {
      display: inline-block; width: 22px; height: 22px; line-height: 22px;
      text-align: center; border-radius: 50%; margin-right: 6px; font-size: 11px; font-weight: 1000;
    }
    .rank-1 { background: #ffd700; color: #000; }
    .rank-2 { background: #c0c0c0; color: #000; }
    .rank-3 { background: #cd7f32; color: #fff; }
    .rank-other { background: #e9ecef; color: #6c757d; }
    @media (max-width: 768px){
      .controls { flex-direction: column; align-items: stretch; }
      .search-box { max-width: 100%; }
      .model-cell { position: static; min-width: 0; }
    }

    footer{ border-top:1px solid var(--border); background:#fff; padding:28px 18px; color:var(--muted2); }
    .footer-inner{ max-width:var(--max); margin:0 auto; display:flex; flex-wrap:wrap; justify-content:space-between; gap:12px; font-size:13px; }
  </style>

  <script>
(() => {
  let DATA = [
    // Single-Modal, w/o Search
    { category: "single", model: "OpenAI o3-mini", overall: 31.96, read: 53.75, insh: 52.65, stru: 37.11, vef: 13.57, con: 28.45, cov: 33.74, fid: 48.35, sem: 15.47, acc: 90.00, vqa: 12.60 },
    { category: "single", model: "DeepSeek-V3.2", overall: 43.71, read: 75.37, insh: 87.82, stru: 58.16, vef: 19.28, con: 33.34, cov: 45.48, fid: 18.77, sem: 42.19, acc: 83.85, vqa: 12.88 },
    { category: "single", model: "Kimi K2 (Thinking)", overall: 36.91, read: 71.34, insh: 77.27, stru: 47.34, vef: 17.14, con: 23.54, cov: 24.62, fid: 27.20, sem: 42.00, acc: 90.00, vqa: 9.50 },
    { category: "single", model: "Qwen 3 235B (A22B)", overall: 36.04, read: 77.56, insh: 85.74, stru: 54.05, vef: 17.14, con: 35.60, cov: 45.73, fid: 22.98, sem: 20.43, acc: 53.09, vqa: 4.95 },

    // Multimodal, w/o Search
    { category: "multimodal", model: "Qwen 3 VL 235B (A22B)", overall: 35.08, read: 77.01, insh: 86.48, stru: 52.21, vef: 43.57, con: 18.34, cov: 15.25, fid: 10.68, sem: 30.58, acc: 93.52, vqa: 16.98 },
    { category: "multimodal", model: "GPT-4o", overall: 28.62, read: 52.52, insh: 68.41, stru: 40.90, vef: 10.04, con: 10.94, cov: 4.61, fid: 11.89, sem: 24.10, acc: 71.43, vqa: 18.72 },
    { category: "multimodal", model: "GPT-4.1", overall: 36.95, read: 79.34, insh: 89.04, stru: 53.00, vef: 39.29, con: 15.90, cov: 10.06, fid: 5.61, sem: 29.66, acc: 80.56, vqa: 19.92 },
    { category: "multimodal", model: "GPT-4.1 mini", overall: 34.23, read: 71.25, insh: 83.62, stru: 49.60, vef: 12.86, con: 24.20, cov: 25.44, fid: 12.33, sem: 32.62, acc: 89.91, vqa: 13.21 },
    { category: "multimodal", model: "GPT-4.1 nano", overall: 28.07, read: 49.77, insh: 64.82, stru: 37.28, vef: 10.79, con: 18.99, cov: 19.86, fid: 24.42, sem: 27.02, acc: 76.30, vqa: 13.04 },
    { category: "multimodal", model: "GPT-5 mini", overall: 38.49, read: 70.06, insh: 81.73, stru: 47.18, vef: 39.29, con: 20.02, cov: 26.64, fid: 32.61, sem: 33.90, acc: 94.23, vqa: 15.60 },
    { category: "multimodal", model: "GPT-5.1", overall: 32.69, read: 79.34, insh: 89.04, stru: 53.00, vef: 35.71, con: 15.90, cov: 2.30, fid: 13.67, sem: 22.03, acc: 84.29, vqa: 14.32 },
    { category: "multimodal", model: "GPT-5.2", overall: 32.76, read: 69.75, insh: 83.92, stru: 54.31, vef: 46.43, con: 14.00, cov: 1.43, fid: 5.30, sem: 12.83, acc: 50.00, vqa: 9.16 },
    { category: "multimodal", model: "Grok-3", overall: 29.89, read: 75.17, insh: 86.13, stru: 52.24, vef: 20.00, con: 12.57, cov: 5.79, fid: 2.80, sem: 22.18, acc: 68.39, vqa: 13.89 },
    { category: "multimodal", model: "Grok-4 (Fast Reasoning)", overall: 36.10, read: 60.62, insh: 80.49, stru: 52.99, vef: 36.43, con: 17.30, cov: 14.62, fid: 6.12, sem: 28.46, acc: 87.45, vqa: 19.34 },

    // Multimodal, w/ Search
    { category: "multimodal", model: "Claude 4.5 Haiku", overall: 33.67, read: 74.60, insh: 81.80, stru: 53.22, vef: 28.57, con: 17.90, cov: 14.10, fid: 18.56, sem: 25.98, acc: 76.90, vqa: 11.70 },
    { category: "multimodal", model: "Claude 4.5 Sonnet", overall: 33.61, read: 77.63, insh: 82.31, stru: 51.65, vef: 32.14, con: 14.36, cov: 15.09, fid: 16.11, sem: 20.73, acc: 70.13, vqa: 14.41 },
    { category: "multimodal", model: "Claude 4.5 Opus", overall: 33.84, read: 77.81, insh: 83.86, stru: 50.70, vef: 35.00, con: 30.64, cov: 41.14, fid: 21.97, sem: 21.30, acc: 77.21, vqa: 14.75 },
    { category: "multimodal", model: "Gemini 2.5 Flash", overall: 38.40, read: 56.22, insh: 68.58, stru: 55.44, vef: 32.86, con: 25.35, cov: 27.77, fid: 38.30, sem: 40.67, acc: 75.96, vqa: 25.49 },
    { category: "multimodal", model: "Gemini 2.5 Pro", overall: 38.04, read: 80.04, insh: 85.94, stru: 51.44, vef: 38.57, con: 30.18, cov: 28.77, fid: 14.98, sem: 19.47, acc: 92.86, vqa: 12.50 },
    { category: "multimodal", model: "Gemini 3 Flash", overall: 44.43, read: 81.22, insh: 90.22, stru: 52.00, vef: 45.71, con: 31.95, cov: 35.07, fid: 15.42, sem: 36.61, acc: 87.31, vqa: 18.99 },
    { category: "multimodal", model: "Gemini 3 Pro", overall: 44.68, read: 58.05, insh: 75.39, stru: 49.85, vef: 46.43, con: 37.98, cov: 41.85, fid: 6.46, sem: 40.69, acc: 80.44, vqa: 23.15 },

    // Deep Research Agent
    { category: "agent", model: "Tongyi Deep Research (30B-A3B)", overall: 29.02, read: 54.27, insh: 62.67, stru: 40.07, vef: 12.86, con: 25.99, cov: 30.87, fid: 24.25, sem: 20.39, acc: 93.33, vqa: 20.39 },
    { category: "agent", model: "Perplexity Sonar Deep Research", overall: 37.55, read: 62.29, insh: 64.35, stru: 47.80, vef: 27.86, con: 33.12, cov: 41.51, fid: 16.68, sem: 50.79, acc: 87.75, vqa: 21.22 },
    { category: "agent", model: "ChatGPT Deep Research (o3-mini)", overall: 29.50, read: 52.40, insh: 63.61, stru: 37.30, vef: 29.29, con: 10.19, cov: 4.16, fid: 11.07, sem: 27.32, acc: 73.44, vqa: 21.75 },
    { category: "agent", model: "Gemini Deep Research (Gemini 3 Pro)", overall: 49.41, read: 84.53, insh: 89.56, stru: 70.86, vef: 35.71, con: 56.17, cov: 52.84, fid: 31.29, sem: 41.29, acc: 87.54, vqa: 28.45 }
  ];

  let state = { filter: "all", query: "", sortKey: "overall", sortDir: "desc" };
  let tableBody, searchInput, filterBtns, ths;

  function safeNum(x) {
    const n = Number(x);
    return Number.isFinite(n) ? n : -Infinity;
  }

  // global ranks by overall (fixed regardless of filter)
  function computeGlobalRanks() {
    const tmp = [...DATA].sort((a,b) => safeNum(b.overall) - safeNum(a.overall));
    tmp.forEach((r, i) => { r.__rank = i + 1; });
  }

  function getFiltered() {
    const q = state.query.trim().toLowerCase();
    return DATA.filter(row => {
      const okFilter = (state.filter === "all") || (row.category === state.filter);
      const okQuery = !q || String(row.model || "").toLowerCase().includes(q);
      return okFilter && okQuery;
    });
  }

  function sortRows(rows) {
    const { sortKey, sortDir } = state;
    const dir = sortDir === "asc" ? 1 : -1;

    rows.sort((a, b) => {
      if (sortKey === "model") return dir * String(a.model || "").localeCompare(String(b.model || ""));
      if (sortKey === "rank") return dir * (safeNum(a.__rank) - safeNum(b.__rank));
      return dir * (safeNum(a[sortKey]) - safeNum(b[sortKey]));
    });

    return rows;
  }

  function rankClass(r) {
    if (r === 1) return "rank rank-1";
    if (r === 2) return "rank rank-2";
    if (r === 3) return "rank rank-3";
    return "rank rank-other";
  }

  function fmt(x) {
    const n = Number(x);
    return Number.isFinite(n) ? n.toFixed(2) : "‚Äî";
  }

  function getCompanyLogo(modelName) {
    const model = String(modelName || "").toLowerCase();
    if (model.includes("gpt") || model.includes("openai") || model.includes("chatgpt")) {
      return '<img src="ico/gpt.png" alt="OpenAI" class="company-logo" title="OpenAI" />';
    }
    if (model.includes("claude")) {
      return '<img src="ico/Claude.png" alt="Anthropic" class="company-logo" title="Anthropic" />';
    }
    if (model.includes("gemini")) {
      return '<img src="ico/gemini.png" alt="Google" class="company-logo" title="Google" />';
    }
    if (model.includes("deepseek")) {
      return '<img src="ico/deepseek.png" alt="DeepSeek" class="company-logo" title="DeepSeek" />';
    }
    if (model.includes("qwen") || model.includes("tongyi")) {
      return '<img src="ico/qwen.png" alt="Alibaba" class="company-logo" title="Alibaba" />';
    }
    if (model.includes("kimi")) {
      return '<img src="ico/kimi.png" alt="Moonshot AI" class="company-logo" title="Moonshot AI" />';
    }
    if (model.includes("grok")) {
      return '<img src="ico/grok.png" alt="xAI" class="company-logo" title="xAI" />';
    }
    if (model.includes("perplexity") || model.includes("sonar")) {
      return '<img src="ico/sonar.png" alt="Perplexity" class="company-logo" title="Perplexity" />';
    }
    return '<span class="company-logo" title="Unknown">‚Ä¢</span>';
  }

  function render() {
    if (!tableBody) return;

    const rows = getFiltered();
    const sorted = sortRows([...rows]);

    if (!sorted.length) {
      tableBody.innerHTML = `<tr><td colspan="12" style="text-align:center; color:#64748b; padding:18px;">No results</td></tr>`;
      return;
    }

    tableBody.innerHTML = sorted.map(r => `
      <tr data-category="${r.category || ""}">
        <td class="model-cell">
          <span class="${rankClass(r.__rank)}">${r.__rank}</span>
          ${getCompanyLogo(r.model)}
          <span class="model-name">${r.model || ""}</span>
        </td>
        <td>${fmt(r.overall)}</td>
        <td>${fmt(r.read)}</td>
        <td>${fmt(r.insh)}</td>
        <td>${fmt(r.stru)}</td>
        <td>${fmt(r.vef)}</td>
        <td>${fmt(r.con)}</td>
        <td>${fmt(r.cov)}</td>
        <td>${fmt(r.fid)}</td>
        <td>${fmt(r.sem)}</td>
        <td>${fmt(r.acc)}</td>
        <td>${fmt(r.vqa)}</td>
      </tr>
    `).join("");
  }

  function setSortHeaderUI() {
    if (!ths) return;
    ths.forEach(th => th.classList.remove("sort-asc", "sort-desc"));
    const active = ths.find(th => th.dataset.sort === state.sortKey);
    if (active) active.classList.add(state.sortDir === "asc" ? "sort-asc" : "sort-desc");
  }

  function bind() {
    if (filterBtns && filterBtns.length > 0) {
      filterBtns.forEach(btn => {
        btn.addEventListener("click", () => {
          filterBtns.forEach(b => b.classList.remove("active"));
          btn.classList.add("active");
          state.filter = btn.dataset.filter || "all";
          render();
        });
      });
    }

    if (searchInput) {
      searchInput.addEventListener("input", (e) => {
        state.query = e.target.value || "";
        render();
      });
    }

    if (ths && ths.length > 0) {
      ths.forEach(th => {
        th.addEventListener("click", () => {
          const key = th.dataset.sort;
          if (!key) return;

          if (state.sortKey === key) {
            state.sortDir = (state.sortDir === "asc") ? "desc" : "asc";
          } else {
            state.sortKey = key;
            state.sortDir = (key === "model") ? "asc" : "desc";
          }
          setSortHeaderUI();
          render();
        });
      });
    }
  }

  document.addEventListener("DOMContentLoaded", () => {
    computeGlobalRanks();

    tableBody = document.getElementById("tableBody");
    searchInput = document.getElementById("searchInput");
    filterBtns = Array.from(document.querySelectorAll(".filter-btn"));
    ths = Array.from(document.querySelectorAll("th.sortable"));

    bind();
    setSortHeaderUI();
    render();
  });
})();
  </script>
</head>

<body>
  <header>
    <div class="nav">
      <div class="brand">
        <img src="assets/logo.png" alt="Logo" class="brand-logo" />
        
        <span style="background: linear-gradient(135deg, #0f172a 30%, #475569); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: 900; letter-spacing: -0.5px;">
          MMDR-Bench
        </span>
      </div>

      <div class="navlinks">
        <a href="#introduction">Introduction</a>
        <a href="#dataset">Dataset</a>
        <a href="#leaderboard">Leaderboard</a>
        <a href="#experiments">Experiments</a>
        <a href="#contact">Contact</a>
      </div>
    </div>
  </header>

  <div class="hero-wrap" id="top">
    <div class="hero">
      <div class="container is-max-desktop has-text-centered">
        <div class="mark">
          <img src="assets/logo.png" alt="Logo" class="hero-logo" />
          
          <span style="background: linear-gradient(135deg, #4f46e5, #06b6d4); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">
            MMDR
          </span>
        </div>

        <h1 class="title">
          MMDeepResearch-Bench: Grounded Evaluation &amp; Alignment for Multimodal Deep Research Agents
        </h1>

        <div class="authors-wrapper">
          <div class="authors">
            Peizhou Huang<sup>*</sup>, Zixuan Zhong<sup>*</sup>, Zhongwei Wan, Donghao Zhou, Samiul Alam, Xin Wang, Zexin Li, Zhihao Dou, Li Zhu,
            Jing Xiong, Chaofan Tao, Yan Xu, Dimitrios Dimitriadis, Tuo Zhang<sup>‚Ä†</sup>, Mi Zhang<sup>‚Ä†</sup>
          </div>
          <div class="team-label">MMDR-Bench Team</div>
          <div class="contribution-notes">
            <span>* Core Contributors</span>
            <span style="margin-left: 12px;">‚Ä† Corresponding Authors</span>
          </div>
        </div>

        <div class="publication-links">
          <a class="button" href="paper.pdf" target="_blank" rel="noopener">
            <span class="icon">üìÑ</span><span>Paper</span>
          </a>
          <a class="button" href="https://github.com/MMDeepResearch-Bench/MMDeepResearch-bench.github.io" target="_blank" rel="noopener">
            <span class="icon">‚≠ê</span><span>Code</span>
          </a>
          <a class="button" href="#dataset">
            <span class="icon">üß©</span><span>Dataset</span>
          </a>
          <a class="button" href="#leaderboard">
            <span class="icon">üèÜ</span><span>Leaderboard</span>
          </a>
        </div>

        <div class="meta-row">
          <span class="badge">140 tasks</span>
          <span class="badge">19 domains</span>
          <span class="badge">Daily + Research regimes</span>
          <span class="badge">Citation-grounded reports</span>
        </div>
      </div>
    </div>
  </div>

  <section id="introduction">
    <div class="container is-max-desktop">
      <h2 class="has-text-centered">Introduction</h2>
      
      <div class="content has-text-justified">
        <p>
          Recent advancements in foundation models have driven a paradigm shift from static, language-centric systems to <strong>Large Multimodal Models (LMMs)</strong> capable of jointly processing text and visual inputs. While modern LMMs can reason over structured artifacts like charts and documents, they remain limited by fixed parametric memory. This limitation has motivated the rise of <strong>Deep Research Agents (DRAs)</strong>‚Äîsystems that autonomously browse the web, retrieve external evidence, and synthesize long-form reports to answer open-ended questions.
        </p>
        <p>
          However, a critical gap persists in current evaluations. Real-world research is rarely text-only; it inherently requires aligning textual claims with visual evidence such as scientific figures, medical imagery, or financial charts. Existing benchmarks generally fall into two siloed categories: they either focus on <strong>text-only deep research</strong> (ignoring visual evidence) or <strong>short-form multimodal perception</strong> (ignoring the long-horizon synthesis and retrieval required for research reports).
        </p>
        <p>
          <strong>MMDeepResearch-Bench (MMDR-Bench)</strong> bridges this gap. It is the first benchmark designed to evaluate <strong>end-to-end multimodal deep research</strong>. Unlike previous datasets, every task in MMDR-Bench is packaged as an <strong>image-text bundle</strong>, forcing agents to step beyond simple "seeing" and move towards "researching with vision."
        </p>
      </div>

      <div class="figure-card" style="margin-top:24px; margin-bottom: 24px;">
        <figure>
          <div class="figure-media">
            <img class="paper-fig" src="assets/introdanlan.png" alt="Evaluated Dimensions of MMDR-Bench" />
          </div>
          <figcaption>
            <strong>Figure 1:</strong> The Hierarchical Evaluation Framework of MMDR-Bench. We assess agents not just on isolated skills, but on how they integrate atomic foundation capabilities into complex deep research workflows.
          </figcaption>
        </figure>
      </div>

      <div class="content has-text-justified">
        <p>
          As illustrated in Figure 1, MMDR-Bench evaluates agents across two distinct levels of capability. At the <strong>Foundational Level (Atomic)</strong>, we assess the essential building blocks required for research. This begins with robust <strong>Visual Perception</strong> to accurately read visual elements like entities and numbers, paired with effective <strong>Web Search Tools</strong> usage to multi-step browse and collect diverse sources. Furthermore, the system must demonstrate <strong>Long-Context Understanding</strong> to digest large volumes of multi-source inputs while maintaining consistency, all while strictly adhering to <strong>Instruction Following</strong> constraints to produce aligned outputs.
        </p>

        <p>
          Moving to the <strong>Deep Research Level (Integrated)</strong>, the benchmark tests how these atomic skills synthesize into a coherent workflow. Agents must demonstrate <strong>Multimodal Task Understanding</strong> by jointly parsing visual and textual content to identify the research intent. Crucially, they need <strong>Visually-Grounded Planning</strong>, utilizing visual cues to formulate search queries and iteratively refine their research plan. We enforce strict evidence standards through <strong>Citation-Grounded Reasoning</strong>, where agents must link claims to verifiable multimodal sources. Finally, the process culminates in <strong>Long-Form Report Synthesis</strong>, producing a comprehensive, citation-rich multimodal report.
        </p>
        
        <p style="margin-top: 20px;">
          To operationalize this, we propose a comprehensive pipeline including <strong>FLAE</strong> (for report quality), <strong>TRACE</strong> (for citation verification), and <strong>MOSAIC</strong> (for visual integrity), ensuring that strong visual perception translates into reliable, grounded research.
        </p>
      </div>
    </div>
  </section>

  <div class="band" id="dataset">
    <div class="band-inner">
      <h1 class="band-title">Dataset</h1>
    </div>
  </div>

  <section>
    <div class="container is-max-desktop">
      <h2 class="has-text-centered">MMDR-Bench Benchmark</h2>
      
      <div class="content has-text-justified">
        <p>
          MMDR-Bench is meticulously constructed to simulate the full spectrum of modern information-seeking behaviors. It comprises <strong>140 expert-crafted tasks</strong> spanning <strong>19 diverse domains</strong>. Unlike traditional QA datasets, each instance in MMDR-Bench is packaged as an <strong>"Image-Text Bundle"</strong>‚Äîa textual query paired with a set of necessary visual artifacts. This design forces the agent to treat images not just as decoration, but as primary sources of evidence that must be interpreted, verified, and cited in the final report.
        </p>
        <p>
          To ensure the benchmark covers both breadth and depth, we organize tasks into two distinct but complementary regimes: <strong>Daily</strong> and <strong>Research</strong>.
        </p>
      </div>

      <div class="grid" style="margin-top:32px; align-items: start;">
        <div style="grid-column: span 5;">
          <div class="figure-card">
            <figure>
              <div class="figure-media" style="padding: 20px;">
                <img class="paper-fig" src="assets/distribution.svg" alt="Figure 2: Task Distribution across Daily and Research regimes" />
              </div>
              <figcaption>
                <strong>Figure 2:</strong> The distribution of 140 tasks across 19 domains. The benchmark is split into a <strong>Daily Regime (28.6%)</strong> for breadth and a <strong>Research Regime (71.4%)</strong> for depth.
              </figcaption>
            </figure>
          </div>
        </div>
        
        <div style="grid-column: span 7;">
          <h3 class="muted" style="margin-top: 0;">Regime 1: Daily Tasks (28.6%)</h3>
          <p class="muted" style="font-size: 15px;">
            As shown in the inner ring of Figure 2, this regime covers 10 domains such as <em>Health, Entertainment, and Technology</em>. These tasks simulate everyday user needs using loosely structured visuals like <strong>screenshots, photographs, and UI captures</strong>. The challenge here lies in "grounding in reality"‚Äîagents must deal with noisy, real-world images and perform lightweight, verifiable evidence gathering (e.g., checking if a product in a photo is available locally).
          </p>

          <h3 class="muted" style="margin-top: 16px;">Regime 2: Research Tasks (71.4%)</h3>
          <p class="muted" style="font-size: 15px;">
            The majority of the benchmark focuses on analysis-heavy settings, spanning 9 domains including <em>Computer Science, Biomedical Science, and Economics</em>. These tasks utilize information-dense visuals such as <strong>scientific charts, diagrams, architecture plots, and tables</strong>. Success in this regime requires deep synthesis: agents must extract precise parameters from diagrams (e.g., hidden sizes in a neural network) and integrate them with academic literature to produce professional, citation-grounded reports.
          </p>
        </div>
      </div>

      <h3 class="has-text-centered" style="margin-top: 40px; margin-bottom: 20px;">Case Studies: From Daily Life to Deep Science</h3>
      
      <div class="grid">
        <div style="grid-column: span 12;">
          <div class="figure-card">
            <figure>
              <div class="figure-media">
                <img class="paper-fig" src="assets/datasamples.svg" alt="Figure 3: Example tasks illustrating the difference between Daily and Research workflows" />
              </div>
              <figcaption>
                <strong>Figure 3:</strong> Two concrete examples from MMDR-Bench. <strong>Left:</strong> A Health domain task requiring product recognition and local search. <strong>Right:</strong> A Computer Science task requiring architectural analysis and theoretical complexity derivation.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>

      <div class="content has-text-justified" style="margin-top: 20px;">
        <p>
          Figure 3 vividly illustrates the divergence in capability requirements through two concrete examples. In the Health task shown on the left, the user presents a photo of an eye drop bottle along with described symptoms. The agent cannot merely rely on OCR; it must identify the specific medical indications of the product, cross-reference them with the user's condition (dryness and itching), and perform a geo-specific search to find purchasing options in London. This demands a combination of visual entity identification and localized grounding. Conversely, the Computer Science task on the right presents diagrams of a GPT-style architecture. Here, the agent is required to extract technical facts‚Äîsuch as dataflow directions and normalization layers‚Äîdirectly from the pixels and derive theoretical time or memory complexity using open web sources, testing structural visual parsing and multimodal theoretical reasoning.
        </p>
        <p>
          All tasks were iteratively refined by <strong>PhD-level domain experts</strong> to ensure that the visual evidence is strictly necessary‚Äîan agent cannot solve the task by text search alone.
        </p>
      </div>

    </div>
  </section>

  <div class="band" id="leaderboard">
    <div class="band-inner">
      <h1 class="band-title">Leaderboard</h1>
    </div>
  </div>

  <section>
    <div class="container">
      <div class="leaderboard-container">
        <div class="leaderboard-header">
          <h3>üèÜ Model Rankings</h3>
          <p>Overall and per-metric results across evaluated models</p>
        </div>

        <div class="controls">
          <div class="filter-group">
            <label style="font-weight: 900; font-size: 13px; color: var(--muted);">Category:</label>
            <button class="filter-btn active" data-filter="all">All Models</button>
            <button class="filter-btn" data-filter="single">Single-Modal</button>
            <button class="filter-btn" data-filter="multimodal">Multimodal</button>
            <button class="filter-btn" data-filter="agent">Deep Research</button>
          </div>

          <div class="search-box">
            <input type="text" id="searchInput" placeholder="Search models...">
          </div>
        </div>

        <div class="metrics-legend">
          <span class="legend-title">Metric Key:</span>
          <div class="legend-grid">
            <div class="legend-item">
              <span class="legend-badge">FLAE</span>
              <span><strong>Report Quality:</strong> <b>READ</b> (Readability), <b>INSH</b> (Insightfulness), <b>STRU</b> (Structural Completeness)</span>
            </div>
            <div class="legend-item">
              <span class="legend-badge">TRACE</span>
              <span><strong>Citation Grounding:</strong> <b>VEF</b> (Visual Evidence Fidelity), <b>CON</b> (Consistency), <b>COV</b> (Coverage), <b>FID</b> (Fidelity)</span>
            </div>
            <div class="legend-item">
              <span class="legend-badge">MOSAIC</span>
              <span><strong>Visual-Text Alignment:</strong> <b>SEM</b> (Semantic Match), <b>ACC</b> (Data Accuracy), <b>VQA</b> (Complex Reasoning)</span>
            </div>
          </div>
        </div>

        <div class="table-container">
          <table class="leaderboard-table" id="leaderboardTable">
            <thead>
              <tr>
                <th class="sortable" data-sort="model">Model</th>
                <th class="sortable" data-sort="overall">Overall</th>
                <th class="metric-group sortable" data-sort="read" title="Readability">Read.</th>
                <th class="sortable" data-sort="insh" title="Insightfulness">Insh.</th>
                <th class="sortable" data-sort="stru" title="Structure">Stru.</th>
                <th class="metric-group sortable" data-sort="vef" title="Visual Evidence Fidelity">Vef.</th>
                <th class="sortable" data-sort="con" title="Consistency">Con.</th>
                <th class="sortable" data-sort="cov" title="Coverage">Cov.</th>
                <th class="sortable" data-sort="fid" title="Fidelity">Fid.</th>
                <th class="metric-group sortable" data-sort="sem" title="Semantic">Sem.</th>
                <th class="sortable" data-sort="acc" title="Accuracy">Acc.</th>
                <th class="sortable" data-sort="vqa" title="VQA">VQA</th>
              </tr>
            </thead>
            <tbody id="tableBody"></tbody>
          </table>
        </div>
      </div>
    </div>
  </section>

  <div class="band" id="experiments">
    <div class="band-inner">
      <h1 class="band-title">Experiments</h1>
    </div>
  </div>

  <section>
    <div class="container is-max-desktop">
      <h2 class="has-text-centered">Results &amp; Findings</h2>

      <div class="content has-text-justified">
        <p class="muted">
          Our evaluation across 25 state-of-the-art systems reveals significant trade-offs. While models like <strong>Gemini Deep Research</strong> excel in overall evidence aggregation (Coverage), even strong systems struggle with fine-grained visual details (Detail Extraction) and citation stability. The findings below highlight the complex interplay between seeing, searching, and citing.
        </p>
      </div>

      <h3 style="margin-top:24px; font-size:20px;">Key Findings</h3>
      
      <div class="content has-text-justified">
        <h4 style="font-size:16px; margin-bottom:6px;">1. Vision is a Double-Edged Sword: Beneficial Only When Reliable</h4>
        <p class="muted">
          Adding vision capabilities is not a guaranteed win. We compared <strong>Qwen 3 VL 235B</strong> against its text-only counterpart, <strong>Qwen 3 235B (labeled 'Qwen-Max' in Figure 6)</strong>. Surprisingly, while the multimodal model improves general grounding, it introduces a specific error mode: <strong>Detail Extraction failures (DTE)</strong>. This phenomenon is particularly evident when the <strong>text prompt already provides most of the necessary information</strong>. In these cases, while the text-only model correctly follows the text instructions, the Vision-Language model attempts to parse the accompanying images and <strong>accidentally extracts incorrect fine-grained details</strong>‚Äîsuch as misreading a chart axis or a small numerical label. These visual hallucinations override the correct text premises, poisoning the downstream search and synthesis process.
        </p>

        <h4 style="font-size:16px; margin-bottom:6px; margin-top:18px;">2. Multimodal Alignment &ne; Citation Grounding</h4>
        <p class="muted">
          Being good at "seeing" (high VEF or MOSAIC scores) does not mean a model is good at "citing" (high TRACE scores). In the 'Agentic System Comparison' of Figure 6, we compare a standard model (<strong>Gemini Pro</strong>) against its agentic version (<strong>Gemini Deep Research</strong>). While the agent retrieves far more information (higher Coverage), the graph shows a dramatic spike in <strong>Entity Mis-identification (EMI)</strong> errors (see the dark blue bar). The complexity of multi-step search and consolidating overlapping sources causes the agent to lose track of which fact belongs to which entity, leading to "hallucinations by confusion"‚Äîa problem less prevalent in simpler, single-turn workflows.
        </p>

        <h4 style="font-size:16px; margin-bottom:6px; margin-top:18px;">3. Tools Help, But the Backbone Matters Most</h4>
        <p class="muted">
          Agentic workflows can amplify a strong model, but they cannot fix a weak one. For instance, <strong>Gemini Deep Research</strong> (built on a strong Gemini 3 Pro backbone) achieves top-tier performance, whereas agents built on smaller models (like Tongyi 30B) struggle to match even offline large models. Interestingly, large offline models sometimes outperform web-enabled agents on evidence coverage, suggesting that current agent retrieval strategies may still have bottlenecks in surfacing the right information.
        </p>
      </div>

      <div class="figure-card" style="margin-top:24px;">
        <figure>
          <div class="figure-media">
            <img class="paper-fig" src="assets/unified_failure_analysis.svg" alt="Unified failure analysis comparing Text vs. Vision and Base vs. Agent systems" />
          </div>
          <figcaption>
            <strong>Figure 6:</strong> Unified failure mode analysis. <strong>Left:</strong> Adding vision increases Detail Extraction (DTE) errors. <strong>Right:</strong> Agentic systems increase Entity Mis-identification (EMI) due to complex context management.
          </figcaption>
        </figure>
      </div>
    </div>
  </section>

  <section>
    <div class="container is-max-desktop">
      <h2 class="has-text-centered">Qualitative Analysis</h2>
      <div class="content has-text-justified">
        <p class="muted">
          To better understand the performance gaps, we provide a qualitative comparison between a high-scoring report and a low-scoring report on the same research task.
        </p>
      </div>

      <h3 style="margin-top: 32px;">1. Success Case: Grounded & Verifiable</h3>
      <div class="figure-card">
        <figure>
          <div class="figure-media">
            <img class="paper-fig" src="assets/good.png" alt="Example of a good report" />
          </div>
          <figcaption>
            <strong>Figure 4:</strong> A high-quality report generated by Claude-4.5-Opus (Score: 91). The model correctly identifies the visual subject (Buddhist deities), structures the argument logically, and provides accessible, authoritative citations that strictly align with the visual evidence.
          </figcaption>
        </figure>
      </div>

      <h3 style="margin-top: 32px;">2. Failure Case: Hallucination & Dead Links</h3>
      <div class="figure-card">
        <figure>
          <div class="figure-media">
            <img class="paper-fig" src="assets/bad.png" alt="Example of a bad report" />
          </div>
          <figcaption>
            <strong>Figure 5:</strong> A low-quality report generated by DeepSeek-V3.2 (Score: 29). The model fails to recognize the core visual task, generates misleading executive summaries, and cites inaccessible or irrelevant sources (hallucinated links), leading to a disconnect between the text and the evidence.
          </figcaption>
        </figure>
      </div>

    </div>
  </section>

  <section id="contact">
    <div class="container is-max-desktop">
      <h2 class="has-text-centered">Contact</h2>
      <div class="grid" style="margin-top:14px;">
        <div class="card" style="grid-column: span 6;">
          <h3>Maintainers</h3>
          <p class="muted">Add maintainer names / lab / affiliations here.</p>
        </div>
        <div class="card" style="grid-column: span 6;">
          <h3>Email</h3>
          <p class="muted">contact@example.com</p>
        </div>
      </div>
    </div>
  </section>

  <footer>
    <div class="footer-inner">
      <div>¬© 2026 MMDR-Bench</div>
      <div class="muted2">
        <a href="https://github.com/MMDeepResearch-Bench/MMDeepResearch-bench.github.io" target="_blank" rel="noopener">GitHub Pages</a>
      </div>
    </div>
  </footer>
</body>
</html>
